{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#x1f12f; Raquel Pérez & Javier Bejar - APA/GEI/FIB/UPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: C:\\Users\\pacma\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: C:\\Users\\pacma\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Descomentar para actualizar librerias\n",
    "# Si se actualiza alguna libreria hay que reiniciar el notebook\n",
    "# !pip install pandas --upgrade --user --quiet\n",
    "# !pip install numpy --upgrade --user --quiet\n",
    "# !pip install scipy --upgrade --user --quiet\n",
    "# !pip install statsmodels --upgrade --user --quiet\n",
    "# !pip install scikit-learn --upgrade --user --quiet\n",
    "!pip install plotly --upgrade --user --quiet\n",
    "!pip install apafib --upgrade --user --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "init_time = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APA - Laboratorio - Sesión 2 \n",
    "\n",
    "## Visualización/Reducción de dimensionalidad\n",
    "\n",
    "Esta sesión trabajaremos diferentes métodos de visualización/reducción de dimensionalidad\n",
    "\n",
    "El objetivo de este proceso es \n",
    "\n",
    "- Determinar si existen patrones en los datos mediante su inspección en un espacio de dimensionalidad reducida\n",
    "- Determinar si hay redundancia de los datos\n",
    "- Obtener un conjunto de datos con menos dimensiones que permita reducir el coste de ajustar un modelo (posiblemente perdiendo interpretabilidad) y/o hacer más evidentes los patrones para la tarea objetivo\n",
    "\n",
    "Habitualmente realizaremos este paso después del preproceso de los datos principalmente porque los métodos de reducción de dimensionalidad pueden ser sensibles a los valores extremos y no aceptar valores perdidos\n",
    "\n",
    "La mayoría de los métodos de reducción de dimensionalidad reescalan o estandarizan los datos antes de aplicarse o son insensibles a esas transformaciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# sns.set()\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import set_config\n",
    "import warnings\n",
    "\n",
    "set_config(display='text')\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECCIÓN 0: Visualizar es importante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El obtener la estadística básica de los datos no es suficiente para comprenderlos, muchos datos con características muy diferentes pueden tener los mismos valores, es famoso el cuarteto de Anscombe que muestra cuatro conjuntos diferentes que tienen la misma media y varianza, pero que no tienen nada que ver entre sí.\n",
    "\n",
    "Se pueden construir conjuntos de datos que tienen la misma media y varianza que pueden corresponder a datos que tengan propiedades muy diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the example dataset for Anscombe's quartet\n",
    "df = sns.load_dataset(\"anscombe\")\n",
    "\n",
    "# Show the results of a linear regression within each dataset\n",
    "sns.lmplot(\n",
    "    data=df, x=\"x\", y=\"y\", col=\"dataset\", hue=\"dataset\",\n",
    "    col_wrap=2, palette=\"muted\", ci=None,\n",
    "    height=4, scatter_kws={\"s\": 50, \"alpha\": 1}\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/probml/probml-data/main/data/DatasaurusDozen.tsv\"\n",
    "df = pd.read_csv(url, sep=\"\\t\")\n",
    "\n",
    "dataset_names = [\"dino\",\"h_lines\",\"v_lines\",\"x_shape\",\"star\",\"high_lines\",\"dots\",\"circle\",\n",
    "    \"bullseye\",\"slant_up\",\"slant_down\",\"wide_lines\"]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=3, sharex=True, sharey=True, figsize=(16,18));\n",
    "\n",
    "axs = []\n",
    "for row in axes:\n",
    "    for ax in row:\n",
    "        axs.append(ax);\n",
    "        \n",
    "color_map = plt.cm.get_cmap('tab20');\n",
    "df_stat = pd.DataFrame()\n",
    "\n",
    "for i, ax, name in zip(range(len(axs)), axs, dataset_names):\n",
    "    name_index = df[\"dataset\"] == name\n",
    "    data_df = df[name_index]\n",
    "    data_df = data_df.sort_values(by=\"x\")\n",
    "    x = data_df[\"x\"].values.reshape(-1, 1)\n",
    "    y = data_df[\"y\"].values.reshape(-1, 1)\n",
    "    _ = ax.scatter(x, y, s=6, color=color_map(i)); \n",
    "    \n",
    "    df_dataset = pd.DataFrame({f'{name}_x':data_df[\"x\"].values,f'{name}_y':data_df[\"y\"].values })\n",
    "    df_stat = pd.concat([df_stat,df_dataset],axis=1)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat.describe().loc[['count', 'mean', 'std']].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECCIÓN 1: Visualizando el conjunto de datos crabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Campbell estudió los cangrejos de roca del género \"Leptograpsus\" en 1974. Una especie, Leptograpsus variegatus, se había dividido en dos nuevas especies, previamente agrupadas por color (naranja y azul). Los especímenes preservados pierden su color, por lo que se esperaba que las diferencias morfológicas permitieran clasificar el material del museo.\n",
    "\n",
    "Hay datos disponibles sobre 50 especímenes de cada sexo de cada especie (200 en total), recolectados en Fremantle, Australia Occidental. Cada espécimen tiene medidas sobre: ​​el ancho del lóbulo frontal (FL), el ancho trasero (RW), la longitud a lo largo de la línea media del caparazón (CL), el ancho máximo (CW) del caparazón y la profundidad del cuerpo (BD) en mm, además de color (es decir, la especie) y sexo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from apafib import load_crabs\n",
    "    crabs_data = load_crabs()\n",
    "except:\n",
    "    crabs_data = pd.read_csv(\"crabs.csv\", header=0)\n",
    "\n",
    "crabs_data = crabs_data.rename(columns={'sp':'species', 'FL':'Frontal Lobe', 'RW':'Rear Width', \n",
    "                            'CL':'Caparace Midline','CW':'Maximum width', 'BD':'Body Depth'})\n",
    "\n",
    "crabs_data['species'] = crabs_data['species'].map({'B':'Blue', 'O':'Orange'})\n",
    "crabs_data['sex'] = crabs_data['sex'].map({'M':'Male', 'F':'Female'})\n",
    "\n",
    "crabs_data.describe(include='all')\n",
    "crabs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo es separar los 200 cangrejos en cuatro clases, dadas por las configuraciones 2x2 para sexo (macho/hembra) y especie (azul/naranja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crabs_data['class'] =  crabs_data.species + crabs_data.sex\n",
    "\n",
    "crabs_data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.melt(crabs_data, id_vars=['class'], value_vars=['Frontal Lobe', 'Rear Width', \n",
    "                      'Caparace Midline','Maximum width', 'Body Depth'])\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10,10));\n",
    "sns.despine(bottom=True, left=True);\n",
    "sns.stripplot(x=\"value\", y=\"variable\", hue=\"class\", data=df, dodge=True, alpha=.25, zorder=1);\n",
    "sns.pointplot(x=\"value\", y=\"variable\", hue=\"class\", data=df, \n",
    "               dodge=.8 - .8 / 3, join=False, palette=\"dark\",markers=\"d\", scale=.75, ci=None);\n",
    "handles, labels = ax.get_legend_handles_labels();\n",
    "ax.legend(handles[4:], labels[4:], title=\"species\",\n",
    "          handletextpad=0, columnspacing=1, loc=\"lower right\", ncol=3, frameon=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezaremos con una exploración básica del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = ['Frontal Lobe','Rear Width','Caparace Midline','Maximum width','Body Depth']\n",
    "crabs_data[data_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5));\n",
    "crabs_data[data_columns].boxplot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crabs_data.boxplot(column='Caparace Midline', by='class',figsize=(8,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crabs_data[data_columns].hist(figsize=(16,4), layout=(1,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6));\n",
    "sns.histplot(crabs_data,x='Rear Width', hue='class',bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "sns.pairplot(crabs_data, hue='class');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECCIÓN 2: PCA \n",
    "\n",
    "Ahora vamos a usar PCA para hacer una mejor visualización de nuestros datos.\n",
    "\n",
    "PCA se basa en la matriz de covarianza de las variables, asume que los datos estan estandarizados, asi que aplicaremos esa transformación. Esto cambiará los datos para que todas las variables tengan media 0 y desviación estándar 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crabs_standarized = crabs_data.copy()\n",
    "crabs_standarized[data_columns] = StandardScaler().fit_transform(crabs_data[data_columns])\n",
    "crabs_standarized.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método PCA de scikit-learn devolverá la relación de varianza explicada y todos los parámetros importantes relacionados con PCA.\n",
    "\n",
    "Se puede usar la relación de varianza explicada y los valores singulares para decidir cuántos componentes conservar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPCA = PCA().fit(crabs_standarized[data_columns]);\n",
    "\n",
    "print(myPCA.explained_variance_ratio_)\n",
    "print(myPCA.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6));\n",
    "plt.plot(range(1,len(myPCA.singular_values_ )+1),myPCA.singular_values_ ,alpha=0.8,marker='.');\n",
    "y_label = plt.ylabel('Eigenvalues');\n",
    "x_label = plt.xlabel('Componentes');\n",
    "plt.title('Scree plot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6));\n",
    "plt.plot(range(1,len(myPCA.explained_variance_ratio_ )+1),myPCA.explained_variance_ratio_ ,alpha=0.8,marker='.',label=\"Variancia Explicada\");\n",
    "y_label = plt.ylabel('Variancia explicada');\n",
    "x_label = plt.xlabel('Componentes');\n",
    "plt.plot(range(1,len(myPCA.explained_variance_ratio_ )+1),\n",
    "         np.cumsum(myPCA.explained_variance_ratio_),\n",
    "         c='red',marker='.',\n",
    "         label=\"Variancia explicada acumulativa\");\n",
    "plt.legend();\n",
    "plt.title('Porcentaje de variancia explicada por componente');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar también los pesos que le asigna el PCA a cada componente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(myPCA.components_, cmap='seismic', \n",
    "            xticklabels=list(crabs_data.columns[2:-1]),\n",
    "            vmin=-np.max(np.abs(myPCA.components_)),\n",
    "            vmax=np.max(np.abs(myPCA.components_)),\n",
    "            annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos transformar nuestro conjunto de datos utilizando el PCA \"entrenado\".\n",
    "\n",
    "Generalmente, para la visualización, se eligen 2 o 3 componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_crabs = myPCA.transform(crabs_standarized[data_columns])\n",
    "crabs_standarized[['PC1','PC2', 'PC3']] = transformed_crabs[:,:3]\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "_ = sns.scatterplot(x='PC1', y='PC2', hue='class', data=crabs_standarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que PCA hace un buen trabajo con estos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_crabs = {'BlueFemale': 'y', 'BlueMale': 'b', 'OrangeFemale': 'r', 'OrangeMale': 'g'}\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "plt.scatter(crabs_standarized.PC1,\n",
    "            crabs_standarized.PC2,\n",
    "            zs=crabs_standarized.PC3, \n",
    "            depthshade=False, \n",
    "            c=crabs_data['class'].apply(lambda x: colors_crabs[x]), s=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter_3d(crabs_standarized, x='PC1', y='PC2', z='PC3',\n",
    "              color='class')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECCIÓN 3: Locally Linear Embedding\n",
    "\n",
    "Este método proyecta los datos en un espacio de menor dimensión tratando de preservar las distancias entre los vecinos.\n",
    "\n",
    "Entonces, este método se basa en **distancias** locales. Cuando trabajamos con distancias es importante que todas las variables tengan el mismo rango. De lo contrario, una variable con valores grandes podría confundir el modelo.\n",
    "\n",
    "Para evitar estos problemas usaremos el MinMaxScaler esta vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "crabs_min_max = crabs_data.copy()\n",
    "crabs_min_max[data_columns] = MinMaxScaler().fit_transform(crabs_data[data_columns])\n",
    "crabs_min_max.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=15)\n",
    "crabs_lle = lle.fit_transform(crabs_min_max[data_columns])\n",
    "\n",
    "print('Reconstruction error:', lle.reconstruction_error_)\n",
    "crabs_min_max[['LLE1','LLE2']]=crabs_lle\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='LLE1', y='LLE2', hue='class', data=crabs_min_max);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lle = LocallyLinearEmbedding(n_components=3, n_neighbors=15)\n",
    "crabs_lle = lle.fit_transform(crabs_min_max[data_columns])\n",
    "\n",
    "print('Reconstruction error:', lle.reconstruction_error_)\n",
    "crabs_min_max[['LLE1','LLE2', 'LLE3']]=crabs_lle\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "plt.scatter(crabs_min_max.LLE1,\n",
    "            crabs_min_max.LLE2,\n",
    "            zs=crabs_min_max.LLE3, \n",
    "            depthshade=False, \n",
    "            c=crabs_data['class'].apply(lambda x: colors_crabs[x]), s=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(crabs_min_max, x='LLE1', y='LLE2', z='LLE3',\n",
    "              color='class')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECCIÓN 4: T-Stochastic Neighbor Embedding (TSNE)\n",
    "\n",
    "TSNE transforma las distancias entre puntos en probabilidades y luego trata de mantener las mismas distribuciones de probabilidad mientras transforma los datos a un espacio dimensional más bajo.\n",
    "\n",
    "Depende de la inicialización y su resultado puede cambiar entre ejecuciones, también depende **fuertemente** de sus parámetros. Generalmente se usa solo para visualización, no para reducción de dimensionalidad.\n",
    "\n",
    "Los parámetros más importantes de este modelo son:\n",
    "* Perplejidad: Equilibra la atención entre los aspectos locales y globales de los datos. Por lo general, estará entre 5 y 50 y tiene un efecto muy fuerte en la visualización final.\n",
    "* Número de iteraciones.\n",
    "\n",
    "Podéis consultar esta publicación que cuenta los efectos que tienen los parámetros: https://distill.pub/2016/misread-tsne/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando los datos sin procesar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "crabs_tsne = TSNE(n_components=2, perplexity=10,n_iter=2000, init='random').fit_transform(crabs_data[data_columns])\n",
    "crabs_data[['TSNE1','TSNE2']]=crabs_tsne\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='TSNE1', y='TSNE2', hue='class', data=crabs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crabs_tsne = TSNE(n_components=3, perplexity=10,n_iter=2000, init='random').fit_transform(crabs_data[data_columns])\n",
    "crabs_data[['TSNE1','TSNE2','TSNE3']]=crabs_tsne\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "plt.scatter(crabs_data.TSNE1,\n",
    "            crabs_data.TSNE2,\n",
    "            zs=crabs_data.TSNE3, \n",
    "            depthshade=False, \n",
    "            c=crabs_data['class'].apply(lambda x: colors_crabs[x]), s=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando datos escalados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crabs_tsne = TSNE(n_components=2, perplexity=10,n_iter=1000, init='random').fit_transform(crabs_min_max[data_columns])\n",
    "crabs_min_max[['TSNE1','TSNE2',]]=crabs_tsne\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='TSNE1', y='TSNE2', hue='class', data=crabs_min_max);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crabs_tsne = TSNE(n_components=3, perplexity=10,n_iter=2000, init='random').fit_transform(crabs_min_max[data_columns])\n",
    "crabs_min_max[['TSNE1','TSNE2','TSNE3']]=crabs_tsne\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.scatter(crabs_min_max.TSNE1,\n",
    "            crabs_min_max.TSNE2,\n",
    "            zs=crabs_min_max.TSNE3, \n",
    "            depthshade=False, \n",
    "            c=crabs_min_max['class'].apply(lambda x: colors_crabs[x]), s=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando datos estandarizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crabs_tsne = TSNE(n_components=2, perplexity=10,n_iter=1000, init='random').fit_transform(crabs_standarized[data_columns])\n",
    "crabs_standarized[['TSNE1','TSNE2']]=crabs_tsne\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='TSNE1', y='TSNE2', hue='class', data=crabs_standarized);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crabs_tsne = TSNE(n_components=3, perplexity=10,n_iter=3000, init='random').fit_transform(crabs_standarized[data_columns])\n",
    "crabs_standarized[['TSNE1','TSNE2','TSNE3']]=crabs_tsne\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "plt.scatter(crabs_standarized.TSNE1,\n",
    "            crabs_standarized.TSNE2,\n",
    "            zs=crabs_standarized.TSNE3, \n",
    "            depthshade=False, \n",
    "            c=crabs_standarized['class'].apply(lambda x: colors_crabs[x]), s=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializando con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crabs_tsne = TSNE(n_components=2, perplexity=10,n_iter=1000, init='pca').fit_transform(transformed_crabs)\n",
    "crabs_standarized[['TSNE1','TSNE2',]]=crabs_tsne\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='TSNE1', y='TSNE2', hue='class', data=crabs_standarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crabs_tsne = TSNE(n_components=3, perplexity=12,n_iter=4000, init='pca').fit_transform(transformed_crabs)\n",
    "crabs_standarized[['TSNE1','TSNE2','TSNE3']]=crabs_tsne\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "plt.scatter(crabs_standarized.TSNE1,\n",
    "            crabs_standarized.TSNE2,\n",
    "            zs=crabs_standarized.TSNE3, \n",
    "            depthshade=False, \n",
    "            c=crabs_standarized['class'].apply(lambda x: colors_crabs[x]), s=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(crabs_standarized, x='TSNE1', y='TSNE2', z='TSNE3',\n",
    "              color='class')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECCION 5: Escalado Multidimensional\n",
    "\n",
    "Este modelo proyecta los datos a un espacio de menos dimensiones intentando preservar las distancias en el espacio original usando una transformación lineal mediante mínimos cuadrados.\n",
    "\n",
    "El atributo stress_ del modelo indica el MSE del ajuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "mds = MDS(n_components=2,n_init=15,metric=True)\n",
    "crabs_mds = mds.fit_transform(crabs_min_max[data_columns])\n",
    "crabs_min_max[['MDS1','MDS2']]=crabs_mds\n",
    "print('MSE:', mds.stress_)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='MDS1', y='MDS2', hue='class', data=crabs_min_max);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mds = MDS(n_components=3,n_init=15,metric=True)\n",
    "crabs_mds = mds.fit_transform(crabs_min_max[data_columns])\n",
    "crabs_min_max[['MDS1','MDS2','MDS3']]=crabs_mds\n",
    "print('MSE:', mds.stress_)\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "plt.scatter(crabs_min_max.MDS2,\n",
    "            crabs_min_max.MDS1,\n",
    "            zs=crabs_min_max.MDS3, \n",
    "            depthshade=False, \n",
    "            c=crabs_min_max['class'].apply(lambda x: colors_crabs[x]), s=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECCION 6: ISOMAP\n",
    "\n",
    "Este modelo es una extensión de escalado multidimensional que trata de mantener las distancias geodésicas entre todos los puntos. La distancia geodésica es la distancia más corta entre dos puntos en una superficie. Si la superficie es un plano sería igual a la distancia euclidea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "isomap = Isomap(n_components=2,n_neighbors=10)\n",
    "crabs_isomap = isomap.fit_transform(crabs_min_max[data_columns])\n",
    "crabs_min_max[['Isomap1','Isomap2']]=crabs_isomap\n",
    "print('Reconstruction error:', isomap.reconstruction_error())\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='Isomap1', y='Isomap2', hue='class', data=crabs_min_max);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomap = Isomap(n_components=3,n_neighbors=10)\n",
    "crabs_isomap = isomap.fit_transform(crabs_min_max[data_columns])\n",
    "crabs_min_max[['Isomap1','Isomap2','Isomap3']]=crabs_isomap\n",
    "print('Reconstruction error:', isomap.reconstruction_error())\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "plt.scatter(crabs_min_max.Isomap1,\n",
    "            crabs_min_max.Isomap2,\n",
    "            zs=crabs_min_max.Isomap3, \n",
    "            depthshade=False, \n",
    "            c=crabs_standarized['class'].apply(lambda x: colors_crabs[x]), s=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECCIÓN 7: Fisher Discriminant Analysis \n",
    "\n",
    "Proyecta los datos de manera que se maximice la separabilidad entre clases. **Este modelo es supervisado**.\n",
    "\n",
    "El espacio que podemos obtener esta limitado al numero de clases -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis(n_components=2)\n",
    "crabs_fda = lda_model.fit_transform(crabs_standarized[data_columns].values,y=crabs_standarized['class'])\n",
    "\n",
    "crabs_standarized[['FDA1','FDA2']] = crabs_fda\n",
    "\n",
    "print('Priors:')\n",
    "pd.DataFrame(lda_model.priors_)\n",
    "print('Means:')\n",
    "pd.DataFrame(lda_model.means_)\n",
    "print('Coefs:')\n",
    "pd.DataFrame(lda_model.coef_)\n",
    "print('Explained Variance Ratio')\n",
    "pd.DataFrame(lda_model.explained_variance_ratio_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LinearDiscriminantAnalysis(n_components=3)\n",
    "crabs_fda = lda_model.fit_transform(crabs_standarized[data_columns].values,y=crabs_standarized['class'])\n",
    "\n",
    "crabs_standarized[['FDA1','FDA2', 'FDA3']] = crabs_fda\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='FDA1', y='FDA2', hue='class', data=crabs_standarized);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "plt.scatter(crabs_standarized.FDA1,\n",
    "            crabs_standarized.FDA2,\n",
    "            zs=crabs_standarized.FDA3, \n",
    "            depthshade=False, \n",
    "            c=crabs_standarized['class'].apply(lambda x: colors_crabs[x]), s=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(crabs_standarized, x='FDA1', y='FDA2', z='FDA3',\n",
    "              color='class')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sección 8: Datos no tabulares\n",
    "\n",
    "Los datos tabulares pueden mostrar patrones al reducir su dimensionalidad. Otros tipos de datos como por ejemplo imagenes o texto también se pueden transformar para usarlos mediante métodos de reducción de dimensionalidad convirtiendolos en matrices de valores numéricos\n",
    "\n",
    "\n",
    "## Imagenes\n",
    "\n",
    "El conjunto de datos digits corresponden a imagenes de digitos escritos a mano. \n",
    "\n",
    "Las imágenes tienen una gran dimensionalidad, por lo que son complicadas de tratar en muchos de los modelos de ML que veremos, una posibilidad es aplicarles métodos de reduccion de dimensionalidad. Habitualmente la dimensionalidad del espacio en el que están la imágenes no se demasiado grande. En este conjunto de imágenes tenemos 64 pixeles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd, yd = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar las imagenes redimensionandolas a una matriz cuadrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8));\n",
    "plt.imshow(Xd[0].reshape(8,8), cmap='Greys');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que son imagenes podemos simplemente escalarlas y centrarlas segun el valor mayor de los pixeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd_s= (Xd-8.0)/16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_pca = PCA().fit(Xd_s);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6));\n",
    "plt.plot(range(1,len(digits_pca.explained_variance_ratio_ )+1),digits_pca.explained_variance_ratio_ ,alpha=0.8,marker='.',label=\"Variancia Explicada\");\n",
    "y_label = plt.ylabel('Variancia explicada');\n",
    "x_label = plt.xlabel('Componentes');\n",
    "plt.plot(range(1,len(digits_pca.explained_variance_ratio_ )+1),\n",
    "         np.cumsum(digits_pca.explained_variance_ratio_),\n",
    "         c='red',marker='.',\n",
    "         label=\"Variancia explicada acumulativa\");\n",
    "plt.legend();\n",
    "plt.title('Porcentaje de variancia explicada por componente');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que los primeros componentes contribuyen proporcionalmente mucho mas que el resto y tenemos bastante parte de la variancia con una fraccion del numero original de dimensiones.\n",
    "\n",
    "En este caso como son imagenes podemos visualizar el peso que le asigna PCA a los pixeles en cada componente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=True, figsize=(12,8));\n",
    "\n",
    "_ = axes[0,0].imshow(digits_pca.components_[0].reshape(8,8),cmap='Greys'); \n",
    "_ = axes[0,1].imshow(digits_pca.components_[1].reshape(8,8),cmap='Greys'); \n",
    "_ = axes[0,2].imshow(digits_pca.components_[2].reshape(8,8),cmap='Greys'); \n",
    "_ = axes[1,0].imshow(digits_pca.components_[3].reshape(8,8),cmap='Greys'); \n",
    "_ = axes[1,1].imshow(digits_pca.components_[4].reshape(8,8),cmap='Greys'); \n",
    "_ = axes[1,2].imshow(digits_pca.components_[5].reshape(8,8),cmap='Greys'); \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Visualizando las dos primeras componentes podemos ver que los digitos estan separados mas o menos por clases solo con esas dos dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = digits_pca.transform(Xd_s)\n",
    "data = pd.DataFrame({'PC1':X_pca[:,0], 'PC2':X_pca[:,1],'PC3':X_pca[:,2],'class':yd})\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='class', data=data, palette='tab10');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(data, x='PC1', y='PC2', z='PC3',\n",
    "              color='class')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con t-SNE tenemos una separación casi perfecta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne = TSNE(n_components=2, perplexity=10,n_iter=5000, init='pca', n_jobs=-1).fit_transform(Xd_s)\n",
    "data = pd.DataFrame({'TSNE1':X_tsne[:,0], 'TSNE2':X_tsne[:,1],'class':yd})\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='TSNE1', y='TSNE2', hue='class', data=data, palette='tab10');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con LLE tambien hay cierta separacion, pero como en PCA solo con dos dimensiones no es suficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=13)\n",
    "X_lle = lle.fit_transform(Xd_s)\n",
    "\n",
    "data = pd.DataFrame({'LLE1':X_lle[:,0], 'LLE2':X_lle[:,1],'class':yd})\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='LLE1', y='LLE2', hue='class', data=data, palette='tab10');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lle = LocallyLinearEmbedding(n_components=3, n_neighbors=13)\n",
    "X_lle = lle.fit_transform(Xd_s)\n",
    "\n",
    "data = pd.DataFrame({'LLE1':X_lle[:,0], 'LLE2':X_lle[:,1],'LLE3':X_lle[:,2],'class':yd})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(data, x='LLE1', y='LLE2', z='LLE3',\n",
    "              color='class')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texto\n",
    "\n",
    "El conjunto de datos **20 newsgroups** esta formado por textos de grupos de noticias/grupos de discusion de Usenet (un equivalente al actual reddit de la primera epoca de internet). Esta formado por mensajes de 20 categorías diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionaremos un subconjunto de 6 categorias para experimentar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'comp.graphics', 'sci.space','sci.crypt','rec.sport.baseball','misc.forsale']\n",
    "Xn, yn = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'),categories =categories,return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xn[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La transformación de texto a vectores habitualemente se hace haciendo un cálculo sobre las palabras que contienen los textos. Se pueden hacer diferentes preprocesos sobre el texto para limpiarlo y reducir el número de palabras a solo las que aportan significado, en este caso solamente eliminaremos los denominados stop words.\n",
    "\n",
    "Para hacer el calculo primero usaremos cierta cantidad de las palabras mas frecuentes como atributos y usaremos como valores su frecuencia en los documentos.\n",
    "\n",
    "Existen multiples maneras de normalizar estos vectores, podemos escalarlos, estandarizarlos, convertirlos en vectores de norma unitaria, ... Cada transformación nos dara probalemente resultados diferentes, sera un hiperparametro mas a explorar si lo usamos para obtener el conjunto de datos\n",
    "\n",
    "Convertiremos los vectores de palabras a vectores de norma unitaria por ejemplo. Esto lo que hace es proyectarlos sobre una hiperesfera de radio 1. Podemos probar con otras normalizaciones y obtendremos resultados diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xn_v = CountVectorizer(max_features=500, stop_words=\"english\").fit_transform(Xn)\n",
    "Xn_s = Normalizer().fit_transform(np.asarray(Xn_v.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_pca = PCA().fit(Xn_s);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6));\n",
    "plt.plot(range(1,len(news_pca.explained_variance_ratio_ )+1),news_pca.explained_variance_ratio_ ,alpha=0.8,marker='.',label=\"Variancia Explicada\");\n",
    "y_label = plt.ylabel('Variancia explicada');\n",
    "x_label = plt.xlabel('Componentes');\n",
    "plt.plot(range(1,len(news_pca.explained_variance_ratio_ )+1),\n",
    "         np.cumsum(news_pca.explained_variance_ratio_),\n",
    "         c='red',marker='.',\n",
    "         label=\"Variancia explicada acumulativa\");\n",
    "plt.legend();\n",
    "plt.title('Porcentaje de variancia explicada por componente');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que la variancia se distribuye entre todos los atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = news_pca.transform(Xn_s)\n",
    "data = pd.DataFrame({'PC1':X_pca[:,0], 'PC2':X_pca[:,1],'class':yn})\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='class', data=data, palette='tab10');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aun asi tenemos cierta separabilidad entre las diferentes clases proyectando por los dos primeros componentes.\n",
    "\n",
    "El t-SNE nos puede dar resultados muy diferentes dependiendo de la inicializacion y los parametros que usemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne = TSNE(n_components=2, perplexity=30, early_exaggeration=15, n_iter=1000, metric='cosine', init='pca', n_jobs=-1).fit_transform(Xn_s)\n",
    "data = pd.DataFrame({'TSNE1':X_tsne[:,0], 'TSNE2':X_tsne[:,1],'class':yn})\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='TSNE1', y='TSNE2', hue='class', data=data, palette='tab10');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos pasara algo parecido con LLE dependiendo del numero de vecinos que empleemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=55)\n",
    "X_lle = lle.fit_transform(Xn_s)\n",
    "\n",
    "data = pd.DataFrame({'LLE1':X_lle[:,0], 'LLE2':X_lle[:,1],'class':yn})\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='LLE1', y='LLE2', hue='class', data=data, palette='tab10');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opcion para obtener valores para los atributos es tener en cuenta la frecuencia de las palabras, pero relativa al numero de veces que aparecen en el conjunto de documentos. TF = term frequency, IDF=Inverse document frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xn_v = TfidfVectorizer(max_features=500, stop_words=\"english\").fit_transform(Xn)\n",
    "Xn_s = Normalizer().fit_transform(np.asarray(Xn_v.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_pca = PCA().fit(Xn_s);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6));\n",
    "plt.plot(range(1,len(news_pca.explained_variance_ratio_ )+1),news_pca.explained_variance_ratio_ ,alpha=0.8,marker='.',label=\"Variancia Explicada\");\n",
    "y_label = plt.ylabel('Variancia explicada');\n",
    "x_label = plt.xlabel('Componentes');\n",
    "plt.plot(range(1,len(news_pca.explained_variance_ratio_ )+1),\n",
    "         np.cumsum(news_pca.explained_variance_ratio_),\n",
    "         c='red',marker='.',\n",
    "         label=\"Variancia explicada acumulativa\");\n",
    "plt.legend();\n",
    "plt.title('Porcentaje de variancia explicada por componente');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = news_pca.transform(Xn_s)\n",
    "data = pd.DataFrame({'PC1':X_pca[:,0], 'PC2':X_pca[:,1],'class':yn})\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='class', data=data, palette='tab10');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne = TSNE(n_components=2, perplexity=30, early_exaggeration=15, n_iter=1000, metric='cosine', init='pca', n_jobs=-1).fit_transform(Xn_s)\n",
    "data = pd.DataFrame({'TSNE1':X_tsne[:,0], 'TSNE2':X_tsne[:,1],'class':yn})\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='TSNE1', y='TSNE2', hue='class', data=data, palette='tab10');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=17)\n",
    "X_lle = lle.fit_transform(Xn_s)\n",
    "\n",
    "data = pd.DataFrame({'LLE1':X_lle[:,0], 'LLE2':X_lle[:,1],'class':yn})\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x='LLE1', y='LLE2', hue='class', data=data, palette='tab10');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Running time {timedelta(seconds=(time() - init_time))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
